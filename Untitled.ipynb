{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54cb4bd-f9ef-4026-91cd-7bb3099ed826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)  \n",
    "print(\"CUDA Available:\", torch.cuda.is_available())  \n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a188495-a817-4acc-8fa8-7793eff7dd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average inference time on CPU: 0.9786 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average inference time on GPU: 0.3106 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Define input text\n",
    "input_text = \"The future of AI is\"\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure input tensors are on the same device as the model\n",
    "input_tokens = {key: value.to(device) for key, value in input_tokens.items()}\n",
    "\n",
    "# Function to measure inference time\n",
    "def benchmark_inference(model, input_tokens, device, num_runs=10):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_time = 0.0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            output = model.generate(**input_tokens, max_length=50)  # Generate text\n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    print(f\"\\nAverage inference time on {device}: {avg_time:.4f} seconds\")\n",
    "\n",
    "# Run benchmark on CPU\n",
    "model.to(\"cpu\")\n",
    "input_tokens = {key: value.to(\"cpu\") for key, value in input_tokens.items()}  # Move inputs to CPU\n",
    "benchmark_inference(model, input_tokens, \"CPU\")\n",
    "\n",
    "# Run benchmark on GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "    input_tokens = {key: value.to(\"cuda\") for key, value in input_tokens.items()}  # Move inputs to GPU\n",
    "    benchmark_inference(model, input_tokens, \"GPU\")\n",
    "else:\n",
    "    print(\"\\nCUDA not available, skipping GPU test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80992b01-1b33-42fb-98d7-d84944203123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 GPT-2 model successfully converted to ONNX format: gpt2_model.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define example input text\n",
    "input_text = \"The future of AI is\"\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Convert model to ONNX format\n",
    "onnx_model_path = \"gpt2_model.onnx\"\n",
    "\n",
    "# Export the model to ONNX\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    (input_tokens[\"input_ids\"],),  # Model inputs\n",
    "    onnx_model_path,\n",
    "    input_names=[\"input_ids\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input_ids\": {0: \"batch_size\", 1: \"seq_length\"}, \"output\": {0: \"batch_size\"}},\n",
    "    opset_version=14  # ONNX Opset version (must be compatible with ONNX Runtime)\n",
    ")\n",
    "\n",
    "print(f\"\u2705 GPT-2 model successfully converted to ONNX format: {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4d52fe-cbbb-4f13-b5ac-5a48e8c66905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2705 Average inference time with ONNX Runtime on CPU: 0.0112 seconds\n",
      "\n",
      "\u2705 Average inference time with ONNX Runtime on GPU: 0.0058 seconds\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = \"gpt2_model.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Prepare input tokens\n",
    "input_text = \"The future of AI is\"\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"np\")  # Convert to NumPy\n",
    "input_ids = input_tokens[\"input_ids\"].astype(np.int64)  # ONNX requires int64 inputs\n",
    "\n",
    "# Function to run inference with ONNX Runtime\n",
    "def benchmark_onnx_inference(ort_session, input_ids, device, num_runs=10):\n",
    "    total_time = 0.0\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        outputs = ort_session.run(None, {\"input_ids\": input_ids})  # Run inference\n",
    "        end_time = time.time()\n",
    "        total_time += (end_time - start_time)\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    print(f\"\\n\u2705 Average inference time with ONNX Runtime on {device}: {avg_time:.4f} seconds\")\n",
    "\n",
    "# Run benchmark on CPU\n",
    "benchmark_onnx_inference(ort_session, input_ids, \"CPU\")\n",
    "\n",
    "# Run benchmark on GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    ort_session = ort.InferenceSession(onnx_model_path, providers=[\"CUDAExecutionProvider\"])\n",
    "    benchmark_onnx_inference(ort_session, input_ids, \"GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86eaa887-b592-4df0-975c-b2c8fd855b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 GPT-2 model successfully converted to TensorRT format: gpt2_model.trt\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "onnx_model_path = \"gpt2_model.onnx\"\n",
    "trt_model_path = \"gpt2_model.trt\"\n",
    "\n",
    "# Create a TensorRT logger\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "# Create TensorRT builder and network\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "\n",
    "# Read the ONNX model\n",
    "with open(onnx_model_path, \"rb\") as model_file:\n",
    "    if not parser.parse(model_file.read()):\n",
    "        print(\"\u274c Failed to parse ONNX model!\")\n",
    "        for error in range(parser.num_errors):\n",
    "            print(parser.get_error(error))\n",
    "        exit()\n",
    "\n",
    "# Set TensorRT builder configurations\n",
    "config = builder.create_builder_config()\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # Set max workspace size\n",
    "\n",
    "# Enable FP16 optimization (Optional: Can be removed if GPU doesn\u2019t support FP16)\n",
    "if builder.platform_has_fast_fp16:\n",
    "    config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "# Create an optimization profile (for dynamic batch sizes)\n",
    "profile = builder.create_optimization_profile()\n",
    "profile.set_shape(\"input_ids\", (1, 1), (1, 50), (1, 100))  # Min, Opt, Max batch sizes\n",
    "config.add_optimization_profile(profile)\n",
    "\n",
    "# Build and serialize the engine\n",
    "engine = builder.build_serialized_network(network, config)\n",
    "\n",
    "# Ensure the engine was built successfully\n",
    "if engine is None:\n",
    "    raise RuntimeError(\"\u274c Failed to build TensorRT engine!\")\n",
    "\n",
    "# Save the model\n",
    "with open(trt_model_path, \"wb\") as f:\n",
    "    f.write(engine)\n",
    "\n",
    "print(f\"\u2705 GPT-2 model successfully converted to TensorRT format: {trt_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2a65676-cd38-42f5-90ad-6b41d63c4dcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidProtobuf",
     "evalue": "[ONNXRuntimeError] : 7 : INVALID_PROTOBUF : Load model from gpt2_model.trt failed:Protobuf parsing failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidProtobuf\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_tokens\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Run benchmark\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mbenchmark_tensorrt_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m, in \u001b[0;36mbenchmark_tensorrt_inference\u001b[1;34m(engine, input_ids, num_runs)\u001b[0m\n\u001b[0;32m     23\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Normally, TensorRT requires an execution context, here we'll compare it with ONNX Runtime\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrt_engine_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproviders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCUDAExecutionProvider\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28;01mNone\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids})\n\u001b[0;32m     26\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     27\u001b[0m total_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:465\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[1;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:526\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[1;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_ep_custom_ops(session_options, providers, provider_options, available_providers)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n\u001b[1;32m--> 526\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    528\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n",
      "\u001b[1;31mInvalidProtobuf\u001b[0m: [ONNXRuntimeError] : 7 : INVALID_PROTOBUF : Load model from gpt2_model.trt failed:Protobuf parsing failed."
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import time\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "# Load the optimized TensorRT engine\n",
    "trt_engine_path = 'gpt2_model.trt'\n",
    "\n",
    "# Create TensorRT runtime and deserialize engine\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "runtime = trt.Runtime(TRT_LOGGER)\n",
    "with open(trt_engine_path, 'rb') as f:\n",
    "    engine_data = f.read()\n",
    "engine = runtime.deserialize_cuda_engine(engine_data)\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "def benchmark_tensorrt_inference(context, input_ids, num_runs=10):\n",
    "    input_binding = engine.get_binding_index('input_ids')\n",
    "    output_binding = engine.get_binding_index('output')\n",
    "    context.set_binding_shape(input_binding, input_ids.shape)\n",
    "    d_input = cuda.mem_alloc(input_ids.nbytes)\n",
    "    output_shape = tuple(context.get_binding_shape(output_binding))\n",
    "    output_dtype = trt.nptype(engine.get_binding_dtype(output_binding))\n",
    "    output = np.empty(output_shape, dtype=output_dtype)\n",
    "    d_output = cuda.mem_alloc(output.nbytes)\n",
    "    total_time = 0.0\n",
    "    for _ in range(num_runs):\n",
    "        start = time.time()\n",
    "        cuda.memcpy_htod(d_input, input_ids)\n",
    "        context.execute_v2([int(d_input), int(d_output)])\n",
    "        cuda.memcpy_dtoh(output, d_output)\n",
    "        end = time.time()\n",
    "        total_time += end - start\n",
    "    avg_time = total_time / num_runs\n",
    "    print(f'\u2705 Average inference time with TensorRT: {avg_time:.4f} seconds')\n",
    "    return output\n",
    "\n",
    "# Dummy input ids\n",
    "input_ids = np.array([[50256]], dtype=np.int32)\n",
    "benchmark_tensorrt_inference(context, input_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}